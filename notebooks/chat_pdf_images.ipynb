{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with PDF page images\n",
    "\n",
    "**If you're looking or the web application, check the src/ folder.** \n",
    "\n",
    "This notebook demonstrates how to convert PDF pages to images and send them to a vision model for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate to OpenAI\n",
    "\n",
    "The following code connects to OpenAI, either using an Azure OpenAI account, GitHub models, or local Ollama model. See the README for instruction on configuring the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import azure.identity\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "openai_host = os.getenv(\"OPENAI_HOST\")\n",
    "if openai_host == \"local\":\n",
    "    # Use a local endpoint like llamafile server\n",
    "    print(\"Using local OpenAI-compatible API with no key\")\n",
    "    openai_client = openai.OpenAI(api_key=\"no-key-required\", base_url=os.getenv(\"LOCAL_OPENAI_ENDPOINT\"))\n",
    "elif openai_host == \"github\":\n",
    "    print(\"Using GitHub-hosted model\")\n",
    "    openai_client = openai.OpenAI(\n",
    "        api_key=os.environ[\"GITHUB_TOKEN\"],\n",
    "        base_url=os.environ[\"GITHUB_MODELS_ENDPOINT\"],\n",
    "    )\n",
    "elif os.getenv(\"AZURE_OPENAI_KEY\"):\n",
    "    # Authenticate using an Azure OpenAI API key\n",
    "    # This is generally discouraged, but is provided for developers\n",
    "    # that want to develop locally inside the Docker container.\n",
    "    print(\"Using Azure OpenAI with key\")\n",
    "    openai_client = openai.AzureOpenAI(\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-02-15-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )\n",
    "elif os.getenv(\"AZURE_OPENAI_ENDPOINT\"):\n",
    "    # Authenticate using the default Azure credential chain\n",
    "    # See https://docs.microsoft.com/azure/developer/python/azure-sdk-authenticate#defaultazurecredential\n",
    "    # This will *not* work inside a local Docker container.\n",
    "    # If using managed user-assigned identity, make sure that AZURE_CLIENT_ID is set\n",
    "    # to the client ID of the user-assigned identity.\n",
    "    print(\"Using Azure OpenAI with default credential\")\n",
    "    default_credential = azure.identity.DefaultAzureCredential(exclude_shared_token_cache_credential=True)\n",
    "    token_provider = azure.identity.get_bearer_token_provider(\n",
    "        default_credential, \"https://cognitiveservices.azure.com/.default\"\n",
    "    )\n",
    "    openai_client = openai.AzureOpenAI(\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-02-15-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDFs to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow pypdf PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "from PIL import Image\n",
    "from pypdf import PdfReader\n",
    "\n",
    "filename = \"plants.pdf\"\n",
    "with open(filename, \"rb\") as reopened_file:\n",
    "    reader = PdfReader(reopened_file)\n",
    "    page_count = len(reader.pages)\n",
    "\n",
    "doc = pymupdf.open(filename)\n",
    "for i in range(page_count):\n",
    "    doc = pymupdf.open(filename)\n",
    "    page = doc.load_page(i)\n",
    "    pix = page.get_pixmap()\n",
    "    original_img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    original_img.save(f\"page_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send images to vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def open_image_as_base64(filename):\n",
    "    with open(filename, \"rb\") as image_file:\n",
    "        image_data = image_file.read()\n",
    "    image_base64 = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "    return f\"data:image/png;base64,{image_base64}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = [\n",
    "    {\"text\": \"What plants are listed on these pages?\", \"type\": \"text\"}\n",
    "]\n",
    "for i in range(page_count):\n",
    "    user_content.append({\"image_url\": {\"url\": open_image_as_base64(f\"page_{i}.png\")}, \"type\": \"image_url\"})\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=os.environ[\"OPENAI_MODEL\"],\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
